{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chris Tanasescu (Margento): Networks of Texts (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#The OS module in Python provides a way of using operating system dependent functionality. \n",
    "#The functions that the OS module provides allows you to interface with the underlying operating system \n",
    "#that Python is running on – be that Windows, Mac or Linux.\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import logging\n",
    "#Module that records events related to the application’s operation. \n",
    "#The log record, which is created with every logging event, contains readily available diagnostic information such as \n",
    "#the file name, full path, function, and line number of the logging event.\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#sklearn implements support vector classification; it is part of \n",
    "#scikit-learn, a free software machine learning library for Python (tools for data mining and data analysis)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "#nltk.download('popular', halt_on_error=False)\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('stop_words_poetry.txt')\n",
    "\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"'d\")\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"&\")\n",
    "stopwords.append(\"upon\")\n",
    "stopwords.append(\"also\")\n",
    "stopwords.append(\"hath\")\n",
    "stopwords.append(\"must\")\n",
    "stopwords.append(\"therefore\")\n",
    "stopwords.append(\"doth\")\n",
    "stopwords.append(\"could\")\n",
    "stopwords.append(\"would\")\n",
    "#stopwords.append(\"another\")\n",
    "stopwords.append(\"much\")\n",
    "#stopwords.append(\"give\")\n",
    "stopwords.append(\"like\")\n",
    "stopwords.append(\"since\")\n",
    "#stopwords.append(\"many\")\n",
    "#stopwords.append(\"without\")\n",
    "#stopwords.append(\"first\")\n",
    "stopwords.append(\"though\")\n",
    "#stopwords.append(\"well\")\n",
    "#stopwords.append(\"often\")\n",
    "#stopwords.append(\"great\")\n",
    "stopwords.append(\"either\")\n",
    "#stopwords.append(\"even\")\n",
    "stopwords.append(\"shall\")\n",
    "#stopwords.append(\"they\")\n",
    "stopwords.append(\"what\")\n",
    "stopwords.append(\"their\")\n",
    "#stopwords.append(\"more\")\n",
    "#stopwords.append(\"there\")\n",
    "#stopwords.append(\"your\")\n",
    "#stopwords.append(\"them\")\n",
    "stopwords.append(\"’\")\n",
    "stopwords.append(\"“\")\n",
    "stopwords.append(\"2\")\n",
    "stopwords.append(\"3\")\n",
    "stopwords.append(\"”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_clean(list_of_text):\n",
    "        '''\n",
    "        preliminary cleaning of the text\n",
    "        - remove new line character i.e. \\n or \\r\n",
    "        - remove tabs i.e. \\t\n",
    "        - remove extra spaces\n",
    "        '''\n",
    "        cleaned_list = []\n",
    "        for text in list_of_text:\n",
    "            # print(\"original:\", text)\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            text = text.replace('\\\\r', ' ')\n",
    "            text = text.replace('\\\\t', ' ')\n",
    "            pattern = re.compile(r'\\s+')\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "            text = text.strip()\n",
    "            # check for empty strings\n",
    "            if text != '' and text is not None:\n",
    "                cleaned_list.append(text)\n",
    "\n",
    "        return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = _pre_clean(tokens)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    #tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getcwd()\n",
    "\n",
    "TEXTS_DIR = HOME + \"/US_Poets_Anthology2/\"\n",
    "\n",
    "#TEXTS_DIR = HOME\n",
    "\n",
    "filelabels = {}\n",
    "\n",
    "texts_data = []\n",
    "\n",
    "files = [f for f in os.listdir(TEXTS_DIR) if os.path.isfile(os.path.join(TEXTS_DIR, f))]\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "tokens_total = []\n",
    "\n",
    "count = -1\n",
    " \n",
    "os.chdir(TEXTS_DIR)\n",
    "    \n",
    "for f in files:\n",
    "    #os.chdir(TEXTS_DIR)\n",
    "    with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "        tokens = []\n",
    "        count = count + 1\n",
    "        filelabels[count] = os.path.basename(openf.name)\n",
    "        for line in openf:\n",
    "            sent_text = nltk.sent_tokenize(line)\n",
    "            for sentence in sent_text:\n",
    "                tokens1 = tokenize(sentence)\n",
    "                tokens1 = [item.translate(remove_punct_map)\n",
    "                      for item in tokens1]\n",
    "                #filter_object = filter(lambda x: x != \"\", tokens1)\n",
    "                tokens1 = [x for x in tokens1 if x!= \"\"]\n",
    "                tokens1 = [x.lower() for x in tokens1]\n",
    "                for token in tokens1:\n",
    "                    tokens.append(token)\n",
    "                    tokens_total.append(token)\n",
    "                #if random.random() > .99:\n",
    "                #print(tokens)\n",
    "    #print(tokens_total)\n",
    "    texts_data.append(tokens)\n",
    "\n",
    "print(filelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(count) It would be a lot to print\n",
    "\n",
    "import pyperclip as clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.copy(f\"{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command+V into a page/word/txt file [or clip.paste() to print it here, but in this case it is too large a list to print]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['a', 'like', 'you', 'they', 'he', 'be', 'it', 'your', 'her', 'of', 'more', 'there', 'no', 'not', '’', 'what', 'my', 'his', 'she', 'to', 'our', 'me', 'we', 'in', 'can', 'us', 'an', 'if', 'do', 'this', '”', 'because', 'who', 'hand', 'but', 'him'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_total = [x for x in tokens_total if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(filelabels)):\n",
    "    print(len(texts_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filelabels)):\n",
    "    texts_data[i] = [x for x in texts_data[i] if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filelabels)):\n",
    "    print(len(texts_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(path):\n",
    "    os.chdir(path)\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    texts = []\n",
    "    count = -1\n",
    "    for f in files:\n",
    "        with codecs.open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "            count = count + 1\n",
    "            filelabels[count] = os.path.basename(openf.name)\n",
    "            splitted_lines = openf.read().splitlines()\n",
    "            splitted_lines = _pre_clean(splitted_lines)\n",
    "            texts.append(splitted_lines)\n",
    "    #print(filelabels)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents(TEXTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "\n",
    "for document in documents:\n",
    "    new_document = \"\"\n",
    "    for string_ in document:\n",
    "        exclude = set(string.punctuation)\n",
    "        string_ = ''.join(ch for ch in string_ if ch not in exclude)\n",
    "        lower_string = string_.lower()\n",
    "        new_document = \" \".join([new_document, lower_string]) \n",
    "    new_documents.append(new_document)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_documents[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_documents[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(new_documents, stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "tfidf = tf_idf_vect.fit_transform(new_documents)\n",
    "\n",
    "#print(type(tfidf))\n",
    "\n",
    "W = tfidf.toarray()\n",
    "\n",
    "#print(type(W))\n",
    "\n",
    "dt = [('correlation', float)]\n",
    "\n",
    "similarity_matrix = np.matrix((tfidf * tfidf.T).A, dtype=dt)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.from_numpy_matrix(similarity_matrix)\n",
    "\n",
    "weights = [(G[tpl[0]][tpl[1]]['correlation']) for tpl in G.edges()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [(x, x) for x in G.nodes()] \n",
    "\n",
    "G.remove_edges_from(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G):\n",
    "    weights = [(G[tpl[0]][tpl[1]]['correlation']) for tpl in G.edges()]\n",
    "    normalized_weights = [400*weight/sum(weights) for weight in weights]\n",
    "    fig, ax = plt.subplots(figsize=(25, 16))\n",
    "    pos=nx.spring_layout(G)\n",
    "    nx.draw_networkx(\n",
    "        G,\n",
    "        pos,\n",
    "        edges=G.edges(),\n",
    "        width=normalized_weights,\n",
    "        with_labels=True,\n",
    "        node_size=70,\n",
    "        node_color='r',\n",
    "        alpha=1,\n",
    "        font_size=22\n",
    "    )\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Count will print the tokens and their number of occurrences throughout the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(count))  #tokens counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So the dimensions of X and Y are the same. What is the difference between them?\n",
    "\n",
    "Les dimensions de X et Y sont donc les mêmes. Quelle est la différence entre elles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Also, why is there a difference between the dimension of Y and that of the list of tokens?  [doc vs new_doc?]\n",
    "\n",
    "Par ailleurs, pourquoi y a-t-il une différence entre la dimension de Y et celle de la liste de tokens ? [doc vs new_doc ?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How can we find out which word corresponds to a certain index in X or Y?\n",
    "\n",
    "Comment savoir quel mot correspond à un certain indice dans X ou Y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[7736]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[8001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How can we find out which indices are not zero for a certain document \n",
    "(and thus which of the whole huge set of tokens occur in that document)?\n",
    "\n",
    "Comment pouvons-nous savoir quels indices ne sont pas nuls pour un certain document \n",
    "(et donc lesquels de l'énorme ensemble de tokens apparaissent dans ce document) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j = 0\n",
    "ind0 = []\n",
    "for i in range(22961):\n",
    "    if Y[0][i] != 0: \n",
    "        ind0.append(i) \n",
    "        j += 1\n",
    "    else:\n",
    "        pass\n",
    "print(j) #how many occurence values are not zero in the node number 0\n",
    "print(ind0) #indeces of tokens with their occurence value different from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can we track down which is which (which word/token corresponds to which index) in the list above? Let us first do one more example. \n",
    "\n",
    "Pouvons-nous déterminer qui est qui (quel mot/token correspond à quel index) dans la liste ci-dessus ? Prenons d'abord un autre exemple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Degrees = G.degree()\n",
    "Sorted_degrees = sorted(Degrees, key = lambda t: t[1], reverse = True)\n",
    "Sorted_degrees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "ind4 = []\n",
    "for i in range(22961):\n",
    "    if Y[4][i] != 0: \n",
    "        ind4.append(i) \n",
    "        j += 1\n",
    "    else:\n",
    "        pass\n",
    "print(j) #how many occurence values are not zero in the node number 4\n",
    "print(ind4) #indeces of tokens with their occurence value different from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We worked out the calculus for the document (node) with the highest degree and for another random one.\n",
    "\n",
    "How about the node with the lowest degree?\n",
    "\n",
    "Nous avons effectué le calcul pour le document (nœud) ayant le plus haut degré et pour un autre nœud aléatoire.\n",
    "\n",
    "Qu'en est-il du nœud avec le plus petit degré ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_degrees[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "ind33 = []\n",
    "for i in range(22961):\n",
    "    if Y[33][i] != 0: \n",
    "        ind33.append(i) \n",
    "        j += 1\n",
    "    else:\n",
    "        pass\n",
    "print(j) #how many occurence values are not zero in the 33rd node \n",
    "print(ind33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see here how having a certain position in the graph also speaks \n",
    "to the specific NLP-relevant anatomy of that respective document.\n",
    "\n",
    "Nous pouvons voir ici comment le fait d'avoir une certaine position dans le graphe parle aussi \n",
    "de l'anatomie TAL spécifique du document en question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us now see what are the indices appearing in both lists (ind4 and ind33). \n",
    "We need to compute therefore the intersection of the two sets representing the two lists.\n",
    "\n",
    "#### Voyons maintenant quels sont les indices apparaissant dans les deux listes (ind4 et ind33). \n",
    "Nous devons donc calculer l'intersection des deux ensembles représentant les deux listes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(set(ind4).intersection(ind33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What does this list (of one element) represent? What does the index in it stand for?\n",
    "\n",
    "Que représente cette liste (d'un élément) ? Que représente l'indice qu'elle contient ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(set(ind4).intersection(ind33)):\n",
    "    print(Z[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Is this word (and any other word we look into) of equal importance to the documents it occurs in and to the corpus as a whole?\n",
    "\n",
    "Ce mot (et tout autre mot que nous étudions) a-t-il la même importance pour les documents dans lesquels il apparaît et pour le corpus dans son ensemble ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W is the tfidf array for the whole corpus.\n",
    "W est le tableau tfidf pour l'ensemble du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "ind33 = []\n",
    "for i in range(22961):\n",
    "    if W[33][i] != 0: \n",
    "        ind33.append(i) \n",
    "        j += 1\n",
    "    else:\n",
    "        pass\n",
    "print(j) #how many TFIDF values are not zero in node 33\n",
    "print(ind33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Oops, tuurns out they are the same as in Y. Is that really weird?\n",
    "\n",
    "Oups, il s'avère qu'ils sont les mêmes qu'en Y. C'est vraiment bizarre ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then what can we gain from examining W closely as compared to sticking only with Y?\n",
    "\n",
    "Alors, que pouvons-nous gagner en examinant de près W plutôt que de nous en tenir uniquement à Y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "ind33 = []\n",
    "for i in range(22961):\n",
    "    if W[33][i] != 0: \n",
    "        print((i, W[33][i]))\n",
    "        ind33.append(i) \n",
    "        j += 1\n",
    "    else:\n",
    "        pass\n",
    "print(j) #how many TFIDF values are not zero in node 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "TFIDF33 = []\n",
    "for i in range(22961):\n",
    "    if W[33][i] != 0: \n",
    "        #print(i, W[21][i], Z[i])\n",
    "        TFIDF33.append((i, W[33][i], Z[i])) \n",
    "        j += 1\n",
    "    else:\n",
    "        pass\n",
    "print(TFIDF33)\n",
    "print(j) #how many TFIDF values are not zero in node 33 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CENTRALITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Closeness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a connected graph, closeness centrality (or closeness) of a node is a measure of centrality in a network, calculated as the sum of the length of the shortest paths between the node and all other nodes in the graph. Thus the more central a node is, the closer it is to all other nodes. \n",
    "\n",
    "Dans un graphe connecté, la centralité de proximité (ou proximité) d'un nœud est une mesure de la centralité dans un réseau, calculée comme la somme de la longueur des chemins les plus courts entre le nœud et tous les autres nœuds du graphe. Ainsi, plus un nœud est central, plus il est proche de tous les autres nœuds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clo_cen = nx.closeness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us sort the closeness centralities.\n",
    "Faisons le tri des centralités de proximité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = OrderedDict(sorted(clo_cen.items(), key=lambda t: t[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Closeness centrality for G:\", c)\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way of doing the ordering:\n",
    "import operator \n",
    "c = sorted(clo_cen.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Closeness centrality for G:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.closeness_centrality(G, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.closeness_centrality(G, 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So we have 15 nodes of maximum (closeness) centrality. Previously we found out what word occurs both in node 4 and node 33, we can now do the same kind of investigation for these 15 nodes (or for others having identical, close, or strikingly disparate centralities), thus fusing a network-analysis-based feature with an NLP/vector-informed one. What other possible investigation(s)--prompted by the vector space and network related data--could be pursued?\n",
    "\n",
    "Nous avons donc 15 nœuds de centralité (de proximité) maximale. Auparavant, nous avons découvert quel mot se trouve à la fois dans le nœud 4 et dans le nœud 33, nous pouvons maintenant faire le même genre de recherche pour ces 15 nœuds (ou pour d'autres ayant des centralités identiques, proches ou étonnamment disparates), fusionnant ainsi une caractéristique basée sur l'analyse de réseau avec une caractéristique informée par l'espace vectoriel TAL. Quelle(s) autre(s) investigation(s) possible(s) - poussée(s) par l'espace vectoriel et les données relatives au réseau - pourrait-on mener ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "BETWEENNESS CENTRALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bet_cen = nx.betweenness_centrality(G, weight = weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ooops, it turns out we have not defined the weights yet. Did that affect our closeness centrality output?\n",
    "\n",
    "Oups, il s'avère que nous n'avons pas encore défini les poids. Cela a-t-il affecté notre résultat de centralité de proximité ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us define weight as an attribute.\n",
    "Définissons le poids comme un attribut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = [(G[tpl[0]][tpl[1]]['correlation']) for tpl in G.edges()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G[4][33]['correlation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In graph theory, betweenness centrality is a measure of centrality in a graph based on shortest paths.\n",
    "Betweenness centrality measures the extent to which a vertex lies on paths between other vertices. Vertices with high betweenness may have considerable influence within a network by virtue of their control over information passing between others. They are also the ones whose removal from the network will most disrupt communications between other vertices because they lie on the largest number of paths taken by messages.\n",
    "\n",
    "En théorie des graphes, la centralité d'interdépendance est une mesure de la centralité dans un graphe basée sur les chemins les plus courts.\n",
    "La centralité d'interdépendance mesure la mesure dans laquelle un sommet se trouve sur des chemins entre d'autres sommets. Les sommets ayant une forte centralité d'interdépendance peuvent avoir une influence considérable au sein d'un réseau en raison du contrôle qu'ils exercent sur les informations passant entre les autres. Ils sont également ceux dont la suppression du réseau perturbera le plus les communications entre les autres sommets, car ils se trouvent sur le plus grand nombre de chemins empruntés par les messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bet_cen = nx.betweenness_centrality(G, weight = \"correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = OrderedDict(sorted(bet_cen.items(), key=lambda t: t[1], reverse=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Betweenness centrality for G:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1= list(b)\n",
    "len(b1)\n",
    "\n",
    "print(b1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(b1)):\n",
    "    if b1[i] == 9:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bet = sorted(bet_cen.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Betweenness centrality for G:\", bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bet)):\n",
    "    if bet[i][0] == 9:\n",
    "        print(i, bet[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_top = c[0:21]\n",
    "print(c_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_top = bet[0:21]\n",
    "print(bet_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_bottom = []\n",
    "\n",
    "n = len(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (n-21, n):\n",
    "    c_bottom.append(c[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_top_nodes = []\n",
    "c_bottom_nodes = []\n",
    "\n",
    "for i in range(21):\n",
    "    c_bottom_nodes.append(c_bottom[i][0])\n",
    "    bet_top_nodes.append(bet_top[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_bottom_set = set(c_bottom_nodes)\n",
    "bet_top_set = set(bet_top_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intersect = c_bottom_set.intersection(bet_top_set)\n",
    "print(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in intersect:\n",
    "     for i in range(len(bet)):\n",
    "            if bet[i][0] == item:\n",
    "                print(item, i, bet[i][1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in intersect:\n",
    "     for i in range(len(c)):\n",
    "            if c[i][0] == item:\n",
    "                print(item, i, c[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "EIGENVECTOR CENTRALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In graph theory, eigenvector centrality (also called eigencentrality) is a measure of the influence of a node in a network. Relative scores are assigned to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. A high eigenvector score means that a node is connected to many nodes who themselves have high scores.\n",
    "\n",
    "Google's PageRank and the Katz centrality are variants of the eigenvector centrality.\n",
    "\n",
    "See the math formula here https://bit.ly/2rcP3ie.\n",
    "\n",
    "En théorie des graphes, la centralité des vecteurs propres (également appelée centralité propre) est une mesure de l'influence d'un nœud dans un réseau. Des scores relatifs sont attribués à tous les nœuds du réseau sur la base du concept selon lequel les connexions aux nœuds à score élevé contribuent davantage au score du nœud en question que des connexions égales aux nœuds à faible score. Un score de vecteur propre élevé signifie qu'un nœud est connecté à de nombreux nœuds qui ont eux-mêmes des scores élevés.\n",
    "\n",
    "Le PageRank de Google et la centralité de Katz sont des variantes de la centralité du vecteur propre.\n",
    "\n",
    "Voir la formule mathématique ici https://bit.ly/2rcP3ie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_cen = nx.eigenvector_centrality(G)\n",
    "\n",
    "e = OrderedDict(sorted(eig_cen.items(), key=lambda t: t[1], reverse=True))\n",
    "\n",
    "print(\"Eigenvector centrality for G:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
